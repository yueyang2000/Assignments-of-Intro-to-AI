{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Classification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Tdmtkz-senIwgsm-Dq0lz85PuPOnLvf-","authorship_tag":"ABX9TyPtuyG83PoryJn+w3iP+SZZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aZ1h6a_903Fh","colab_type":"code","outputId":"234d2d2d-93fc-4232-e44d-1efc0aa56686","executionInfo":{"status":"ok","timestamp":1590836341889,"user_tz":-480,"elapsed":5421,"user":{"displayName":"Yang Yue","photoUrl":"","userId":"01852859174863985629"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import sys\n","import datetime\n","from copy import deepcopy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.stats import pearsonr\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uBkEFE2o1V2z","colab_type":"code","colab":{}},"source":["class myDataset(Dataset):\n","    def __init__(self, text, label):\n","        super().__init__()\n","        self.text = torch.tensor(text).to(device)\n","        self.label = torch.tensor(label).to(device)\n","        self.class_idx = torch.argmax(\n","            self.label, dim=1, keepdim=False).to(device)\n","\n","    def __len__(self):\n","        return self.label.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.text[index], self.label[index], self.class_idx[index]\n","\n","\n","class Stat():\n","    def __init__(self, ):\n","        self.loss = []\n","        self.labels = []\n","        self.pred_labels = []\n","        self.classes = []\n","        self.pred_classes = []\n","\n","    def append(self, loss, label, pred_label):\n","        pred_class = pred_label.argmax(dim=1).cpu().detach().numpy()\n","        true_class = label.argmax(dim=1).cpu().numpy()\n","        self.loss.append(loss)\n","        self.labels.extend(label.cpu().detach().numpy())\n","        self.pred_labels.extend(pred_label.cpu().detach().numpy())\n","        self.classes.extend(true_class)\n","        self.pred_classes.extend(pred_class)\n","\n","    def eval(self):\n","        loss = sum(self.loss) / len(self.loss)\n","        acc = accuracy_score(self.classes, self.pred_classes) * 100\n","        f1 = f1_score(self.classes, self.pred_classes,\n","                      average='macro') * 100\n","        corr = sum([pearsonr(self.pred_labels[i], self.labels[i])[0]\n","                    for i in range(len(self.labels))]) / len(self.labels)\n","        return loss, acc, f1, corr\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3lhCfxe-sAo","colab_type":"code","outputId":"16d9453f-b200-4e45-f231-293303c80e0b","executionInfo":{"status":"ok","timestamp":1590836402100,"user_tz":-480,"elapsed":53434,"user":{"displayName":"Yang Yue","photoUrl":"","userId":"01852859174863985629"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(\"====Load Data====\")\n","# colab dir\n","text_train_tensor = np.load('/content/drive/My Drive/PA3/text_train_tensor.npy')\n","label_train_tensor = np.load('/content/drive/My Drive/PA3/label_train_tensor.npy')\n","text_dev_tensor = np.load('/content/drive/My Drive/PA3/text_dev_tensor.npy')\n","label_dev_tensor = np.load('/content/drive/My Drive/PA3/label_dev_tensor.npy')\n","text_test_tensor = np.load('/content/drive/My Drive/PA3/text_test_tensor.npy')\n","label_test_tensor = np.load('/content/drive/My Drive/PA3/label_test_tensor.npy')\n","# test_data = np.load('/content/drive/My Drive/PA3/test_data.npy')\n","# test_label = np.load('/content/drive/My Drive/PA3/test_label.npy')\n","train_set = myDataset(text_train_tensor, label_train_tensor)\n","dev_set = myDataset(text_dev_tensor, label_dev_tensor)\n","test_set = myDataset(text_test_tensor, label_test_tensor)\n","train_loader = DataLoader(\n","    train_set, batch_size=64, shuffle=True)\n","dev_loader = DataLoader(\n","    dev_set, batch_size=64, shuffle=False)\n","test_loader = DataLoader(\n","    dataset=test_set, batch_size=64, shuffle=False)\n","print(\"====Load Finish====\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["====Load Data====\n","====Load Finish====\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3PBRqubp2OQF","colab_type":"code","colab":{}},"source":["\n","max_len = 512\n","embed_size = 300\n","num_class = 8\n","batch_size = 64\n","\n","\n","\n","class MLP(nn.Module):\n","    '''\n","    Multilayer Perceptron\n","    '''\n","    def __init__(self, params):\n","        super().__init__()\n","        hidden_size = params['hidden_size']\n","        dropout = params['dropout']\n","        self.fc = nn.Sequential(\n","            nn.Linear(max_len * embed_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, num_class),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], -1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","class DAN(nn.Module):\n","    '''\n","    Deep Average Network\n","    '''\n","\n","    def __init__(self, params):\n","        super().__init__()\n","        word_dropout = params['word_dropout']\n","        dropout = params['dropout']\n","        hidden_size = params['hidden_size']\n","        self.word_dropout = word_dropout\n","        self.fc = nn.Sequential(\n","            nn.Linear(embed_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, num_class)\n","        )\n","\n","    def forward(self, x):\n","        # x.size: [64, 512, 300]\n","        batch = x.shape[0]\n","        # word dropout\n","        if(self.training):\n","            mask = torch.bernoulli(torch.ones(\n","                batch, max_len) * (1 - self.word_dropout)).to(device)\n","        else:\n","            mask = torch.ones(batch, max_len).to(device)\n","\n","        x[mask == 0] *= 0\n","        word_sum = torch.sum(mask == 1, dim=1).reshape((batch, 1))\n","        x = torch.sum(x, dim=1) / word_sum\n","        x = self.fc(x)\n","        return x\n","\n","\n","class CNN(nn.Module):\n","    '''\n","    Convolutional Neural Network\n","    '''\n","\n","    def __init__(self, params):\n","        super().__init__()\n","        num_filters = params['num_filters']\n","        filter_size = params['filter_size']\n","        dropout = params['dropout']\n","\n","        self.convs = nn.ModuleList([nn.Conv2d\n","                                    (1, num_filters, (k, embed_size)) for k in filter_size])\n","        self.fc = nn.Sequential(\n","            nn.Dropout(dropout),\n","            nn.Linear(num_filters * len(filter_size), num_class)\n","        )\n","\n","    def conv_pool(self, x, conv):\n","        # x: (b, max_len, embed_size)\n","        # conv(x): (b, num_filters, max_len , 1)\n","        x = F.relu(conv(x)).squeeze(3)\n","        x = F.max_pool1d(x, x.shape[2]).squeeze(2)\n","        return x\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x = torch.cat([self.conv_pool(x, conv) for conv in self.convs], 1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","\n","class RNN(nn.Module):\n","    '''\n","    BiLSTM with Self Attention\n","    '''\n","\n","    def __init__(self, params):\n","        super().__init__()\n","        hidden_size = params['hidden_size']\n","        self.Attention = params['Attention']\n","        # input_size, hidden_size\n","        # num_layers default = 1\n","        self.lstm = nn.LSTM(embed_size, hidden_size,\n","                            bidirectional=True, batch_first=True)\n","        if(self.Attention):\n","            self.w = nn.Parameter(torch.zeros(hidden_size * 2))\n","\n","        self.fc = nn.Linear(hidden_size * 2, num_class)\n","\n","    def forward(self, x):\n","        # lstm(x) = output, (h_n, c_n)\n","        # h_0, c_0 default zero\n","        H, _ = self.lstm(x)\n","        if(self.Attention):\n","            M = torch.tanh(H)\n","            alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)\n","            out = H * alpha\n","            out = torch.sum(out, dim=1)\n","            out = self.fc(out)\n","        else:\n","            out = self.fc(H[:, -1, :])\n","        return out\n","\n","class RCNN(nn.Module):\n","    '''\n","    Recurrent Convolutional Neural Network\n","    '''\n","\n","    def __init__(self, params):\n","        super().__init__()\n","        hidden_size = params['hidden_size']\n","        num_filters = params['num_filters']\n","        self.lstm = nn.LSTM(embed_size, hidden_size,\n","                            bidirectional=True, batch_first=True)\n","        self.conv = nn.Conv2d(\n","            1, num_filters, (1, 2 * hidden_size + embed_size))\n","        self.maxpool = nn.MaxPool1d(max_len)\n","        self.fc = nn.Linear(num_filters, num_class)\n","\n","    def forward(self, x):\n","        H, _ = self.lstm(x)\n","        x = torch.cat((x, H), 2).unsqueeze(1)\n","        # [c_l; e; e_r]\n","        x = self.conv(x).squeeze(3)\n","        # x = torch.tanh(x)\n","        x = F.relu(x)\n","        x = F.max_pool1d(x, x.shape[2]).squeeze(2)\n","        x = self.fc(x)\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JF0b1mMd25dj","colab_type":"code","colab":{}},"source":["def run(params):\n","    batch_size = 64\n","    model_type = params['model_type']\n","    lr = params['lr']\n","    num_epochs = params['num_epochs']\n","    weight_decay = params['weight_decay']\n","\n","    model = None\n","    if(model_type == 'MLP'):\n","        model = MLP(params)\n","    elif(model_type == 'DAN'):\n","        model = DAN(params)\n","    elif(model_type == 'CNN'):\n","        model = CNN(params)\n","    elif(model_type == 'RNN'):\n","        model = RNN(params)\n","    elif(model_type == 'RCNN'):\n","        model = RCNN(params)\n","\n","    model.to(device)\n","\n","    print(\"====Train Begin====\")\n","    loss_function = nn.CrossEntropyLoss()\n","    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    best_acc = 0\n","    best_epoch = -1\n","    best_model = None\n","    \n","    \n","\n","    for epoch in range(num_epochs):\n","        print(\"epoch\",epoch)\n","        model.train()\n","        train_stat = Stat()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            text, label, class_idx = batch\n","            pred_label = model(text)\n","\n","            loss = loss_function(pred_label, class_idx)\n","            loss.backward()\n","            optimizer.step()\n","            train_stat.append(loss.item(), label, pred_label)\n","        t_loss, t_acc, t_f1, t_corr = train_stat.eval()\n","        print(\"TRAIN\\tloss:%.4f, acc:%.4f, f1:%.4f, corr:%.4f\" % (\n","            t_loss, t_acc, t_f1, t_corr))\n","\n","        model.eval()\n","        dev_stat = Stat()\n","        with torch.no_grad():\n","            for batch in dev_loader:\n","                text, label, class_idx = batch\n","                pred_label = model(text)\n","                loss = loss_function(pred_label, class_idx)\n","                dev_stat.append(loss.item(), label, pred_label)\n","            d_loss, d_acc, d_f1, d_corr = dev_stat.eval()\n","            print(\"DEV  \\tloss:%.4f, acc:%.4f, f1:%.4f, corr:%.4f\" % (\n","                d_loss, d_acc, d_f1, d_corr))\n","            if(d_acc > best_acc):\n","                best_acc = d_acc\n","                best_model = deepcopy(model.state_dict())\n","                best_epoch = epoch\n","\n","    print(\"====Train End====\")\n","    print(\"best acc:%.4f\" % (best_acc))\n","\n","    torch.save(best_model, '/content/drive/My Drive/PA3/'+ model_type + '.pkl')\n","    model.load_state_dict(best_model)\n","    print(\"====Test====\")\n","    test_stat = Stat()\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            text, label, class_idx = batch\n","            pred_label = model(text)\n","            test_stat.append(0, label, pred_label)\n","        t_loss, t_acc, t_f1, t_corr = test_stat.eval()\n","        print(\"TEST\\tacc:%.4f, f1:%.4f, corr:%.4f\" % (t_acc, t_f1, t_corr))\n","    return t_acc, t_f1, t_corr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbOO_QegagwX","colab_type":"code","colab":{}},"source":["import sys\n","def test_performance(params):\n","    acc_list = []\n","    f1_list = []\n","    corr_list = []\n","    save = sys.stdout\n","    print(params)\n","    for i in range(5):\n","        print(\"run test\",i)\n","        sys.stdout = None\n","        acc, f1, corr = run(params)\n","        sys.stdout = save\n","        acc_list.append(acc)\n","        f1_list.append(f1)\n","        corr_list.append(corr)\n","    acc = sum(acc_list) / 5\n","    f1 = sum(f1_list) / 5\n","    corr = sum(corr_list)/ 5\n","    print(\"PERFORMANCE: acc:%.4f, f1:%.4f, corr:%.4f\" %(acc, f1, corr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JV_hMXCnyxCT","colab_type":"code","outputId":"f40b6a2b-5baf-4f5d-a8a5-2ad801c36dfa","executionInfo":{"status":"ok","timestamp":1590836464145,"user_tz":-480,"elapsed":43268,"user":{"displayName":"Yang Yue","photoUrl":"","userId":"01852859174863985629"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["MLP_params = {\n","    'model_type': 'MLP',\n","    'lr': 1e-3,\n","    'weight_decay': 1e-4,\n","    'num_epochs': 50,\n","    'hidden_size': 512,\n","    'dropout': 0.5\n","}\n","DAN_params = {\n","    'model_type': 'DAN',\n","    'lr': 1e-2,\n","    'weight_decay': 0,\n","    'num_epochs': 100,\n","    'hidden_size': 256,\n","    'dropout': 0.5,\n","    'word_dropout': 0.5\n","}\n","CNN_params = {\n","    'model_type': 'CNN',\n","    'lr': 1e-3,\n","    'weight_decay': 1e-3,\n","    'num_epochs': 20,\n","    'num_filters': 128,\n","    'filter_size': (2, 3, 4),\n","    'dropout': 0.5\n","}\n","RNN_params = {\n","    'model_type': 'RNN',\n","    'lr': 1e-3,\n","    'weight_decay': 0,\n","    'num_epochs': 50,\n","    'hidden_size': 256,\n","    'Attention': True\n","}\n","RCNN_params = {\n","    'model_type': 'RCNN',\n","    'lr': 1e-3,\n","    'weight_decay': 1e-3,\n","    'num_epochs': 20,\n","    'hidden_size': 128,\n","    'num_filters': 128\n","}\n","# test_performance(MLP_params)\n","run(DAN_params)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["====Train Begin====\n","epoch 0\n","TRAIN\tloss:1.6291, acc:43.7325, f1:11.8778, corr:0.3943\n","DEV  \tloss:1.3636, acc:55.6000, f1:16.6520, corr:0.4805\n","epoch 1\n","TRAIN\tloss:1.4552, acc:50.1403, f1:14.9782, corr:0.4609\n","DEV  \tloss:1.2775, acc:57.6000, f1:19.3064, corr:0.5211\n","epoch 2\n","TRAIN\tloss:1.3853, acc:51.8709, f1:17.8804, corr:0.4873\n","DEV  \tloss:1.2302, acc:57.7000, f1:19.5270, corr:0.5756\n","epoch 3\n","TRAIN\tloss:1.3530, acc:53.5547, f1:20.9807, corr:0.5233\n","DEV  \tloss:1.1916, acc:58.8000, f1:22.2374, corr:0.5862\n","epoch 4\n","TRAIN\tloss:1.3291, acc:54.1628, f1:22.4002, corr:0.5287\n","DEV  \tloss:1.1988, acc:59.3000, f1:20.2793, corr:0.5868\n","epoch 5\n","TRAIN\tloss:1.3178, acc:54.0692, f1:22.2399, corr:0.5311\n","DEV  \tloss:1.1718, acc:59.0000, f1:22.5356, corr:0.5806\n","epoch 6\n","TRAIN\tloss:1.2878, acc:55.7063, f1:25.5297, corr:0.5403\n","DEV  \tloss:1.1408, acc:59.5000, f1:23.1911, corr:0.5807\n","epoch 7\n","TRAIN\tloss:1.2541, acc:56.6885, f1:27.3987, corr:0.5400\n","DEV  \tloss:1.1617, acc:59.5000, f1:25.8532, corr:0.5918\n","epoch 8\n","TRAIN\tloss:1.2782, acc:55.9869, f1:26.0501, corr:0.5397\n","DEV  \tloss:1.1895, acc:57.7000, f1:24.9030, corr:0.5868\n","epoch 9\n","TRAIN\tloss:1.2494, acc:56.2208, f1:28.0897, corr:0.5460\n","DEV  \tloss:1.1339, acc:60.8000, f1:28.2812, corr:0.5834\n","epoch 10\n","TRAIN\tloss:1.2460, acc:57.2030, f1:27.9140, corr:0.5426\n","DEV  \tloss:1.1095, acc:60.7000, f1:28.2156, corr:0.5892\n","epoch 11\n","TRAIN\tloss:1.2183, acc:57.4369, f1:28.4878, corr:0.5476\n","DEV  \tloss:1.1185, acc:60.1000, f1:24.7138, corr:0.5917\n","epoch 12\n","TRAIN\tloss:1.2124, acc:57.5304, f1:27.4701, corr:0.5507\n","DEV  \tloss:1.0943, acc:60.4000, f1:29.2968, corr:0.6003\n","epoch 13\n","TRAIN\tloss:1.2356, acc:57.3901, f1:30.0030, corr:0.5467\n","DEV  \tloss:1.1776, acc:57.9000, f1:24.9480, corr:0.5763\n","epoch 14\n","TRAIN\tloss:1.2359, acc:58.5594, f1:31.0934, corr:0.5494\n","DEV  \tloss:1.1158, acc:60.6000, f1:28.7042, corr:0.6054\n","epoch 15\n","TRAIN\tloss:1.1861, acc:57.7175, f1:30.1691, corr:0.5496\n","DEV  \tloss:1.0952, acc:60.7000, f1:26.3750, corr:0.6008\n","epoch 16\n","TRAIN\tloss:1.1845, acc:58.3723, f1:29.5115, corr:0.5545\n","DEV  \tloss:1.1010, acc:60.9000, f1:28.5115, corr:0.5947\n","epoch 17\n","TRAIN\tloss:1.1806, acc:58.4191, f1:30.8973, corr:0.5599\n","DEV  \tloss:1.1138, acc:60.2000, f1:28.3373, corr:0.5945\n","epoch 18\n","TRAIN\tloss:1.1825, acc:58.5594, f1:30.0863, corr:0.5546\n","DEV  \tloss:1.1072, acc:60.6000, f1:28.1496, corr:0.5662\n","epoch 19\n","TRAIN\tloss:1.1779, acc:58.8400, f1:30.1524, corr:0.5423\n","DEV  \tloss:1.1124, acc:61.5000, f1:30.4240, corr:0.5908\n","epoch 20\n","TRAIN\tloss:1.1754, acc:59.7287, f1:32.9308, corr:0.5437\n","DEV  \tloss:1.0947, acc:60.2000, f1:30.8282, corr:0.5795\n","epoch 21\n","TRAIN\tloss:1.1704, acc:58.0917, f1:30.6249, corr:0.5452\n","DEV  \tloss:1.1199, acc:60.1000, f1:28.0220, corr:0.5652\n","epoch 22\n","TRAIN\tloss:1.1537, acc:59.6352, f1:33.5069, corr:0.5480\n","DEV  \tloss:1.0943, acc:61.2000, f1:30.3005, corr:0.5840\n","epoch 23\n","TRAIN\tloss:1.1576, acc:58.5126, f1:31.2384, corr:0.5498\n","DEV  \tloss:1.1032, acc:59.9000, f1:27.0175, corr:0.5865\n","epoch 24\n","TRAIN\tloss:1.1447, acc:60.7577, f1:34.0976, corr:0.5571\n","DEV  \tloss:1.0906, acc:60.8000, f1:28.2986, corr:0.6137\n","epoch 25\n","TRAIN\tloss:1.1472, acc:61.2254, f1:34.4169, corr:0.5621\n","DEV  \tloss:1.1072, acc:61.1000, f1:27.3265, corr:0.5725\n","epoch 26\n","TRAIN\tloss:1.1425, acc:60.9916, f1:35.6472, corr:0.5567\n","DEV  \tloss:1.1023, acc:61.0000, f1:30.0513, corr:0.5797\n","epoch 27\n","TRAIN\tloss:1.1637, acc:59.1207, f1:33.9263, corr:0.5488\n","DEV  \tloss:1.0961, acc:60.3000, f1:27.9001, corr:0.6068\n","epoch 28\n","TRAIN\tloss:1.1613, acc:59.1674, f1:32.9597, corr:0.5555\n","DEV  \tloss:1.1049, acc:60.4000, f1:25.9719, corr:0.5899\n","epoch 29\n","TRAIN\tloss:1.1495, acc:59.5416, f1:33.1637, corr:0.5525\n","DEV  \tloss:1.0997, acc:60.4000, f1:27.3396, corr:0.6081\n","epoch 30\n","TRAIN\tloss:1.1509, acc:59.7755, f1:33.9682, corr:0.5524\n","DEV  \tloss:1.0755, acc:61.5000, f1:30.0345, corr:0.5987\n","epoch 31\n","TRAIN\tloss:1.1365, acc:60.3368, f1:34.9425, corr:0.5489\n","DEV  \tloss:1.1297, acc:60.9000, f1:28.7236, corr:0.5612\n","epoch 32\n","TRAIN\tloss:1.1306, acc:59.8223, f1:35.9215, corr:0.5475\n","DEV  \tloss:1.0771, acc:61.1000, f1:32.8315, corr:0.5962\n","epoch 33\n","TRAIN\tloss:1.1195, acc:59.9158, f1:34.6446, corr:0.5536\n","DEV  \tloss:1.0980, acc:60.8000, f1:29.6488, corr:0.5708\n","epoch 34\n","TRAIN\tloss:1.0972, acc:61.1787, f1:35.5242, corr:0.5477\n","DEV  \tloss:1.0783, acc:61.0000, f1:31.5304, corr:0.5887\n","epoch 35\n","TRAIN\tloss:1.1311, acc:59.5884, f1:34.5864, corr:0.5493\n","DEV  \tloss:1.0897, acc:61.0000, f1:31.5465, corr:0.5731\n","epoch 36\n","TRAIN\tloss:1.1216, acc:59.8690, f1:34.2660, corr:0.5456\n","DEV  \tloss:1.0724, acc:61.2000, f1:31.5065, corr:0.5767\n","epoch 37\n","TRAIN\tloss:1.1128, acc:60.7577, f1:35.1620, corr:0.5437\n","DEV  \tloss:1.0725, acc:61.2000, f1:32.6809, corr:0.5940\n","epoch 38\n","TRAIN\tloss:1.1161, acc:60.5706, f1:36.1470, corr:0.5511\n","DEV  \tloss:1.0700, acc:61.5000, f1:30.8845, corr:0.5948\n","epoch 39\n","TRAIN\tloss:1.0722, acc:62.0674, f1:36.8590, corr:0.5588\n","DEV  \tloss:1.0706, acc:61.7000, f1:33.7575, corr:0.5822\n","epoch 40\n","TRAIN\tloss:1.1269, acc:61.3190, f1:36.1370, corr:0.5361\n","DEV  \tloss:1.0922, acc:60.6000, f1:27.3976, corr:0.5766\n","epoch 41\n","TRAIN\tloss:1.1178, acc:60.4771, f1:34.1690, corr:0.5394\n","DEV  \tloss:1.0759, acc:61.5000, f1:31.3077, corr:0.5740\n","epoch 42\n","TRAIN\tloss:1.1042, acc:60.0094, f1:35.7573, corr:0.5397\n","DEV  \tloss:1.0821, acc:61.0000, f1:30.8189, corr:0.5775\n","epoch 43\n","TRAIN\tloss:1.1042, acc:60.2432, f1:35.9486, corr:0.5465\n","DEV  \tloss:1.0857, acc:62.1000, f1:33.6100, corr:0.5771\n","epoch 44\n","TRAIN\tloss:1.0962, acc:61.6932, f1:36.6044, corr:0.5460\n","DEV  \tloss:1.0797, acc:60.5000, f1:30.6340, corr:0.5811\n","epoch 45\n","TRAIN\tloss:1.0939, acc:60.9916, f1:36.0709, corr:0.5466\n","DEV  \tloss:1.0744, acc:62.1000, f1:32.0742, corr:0.5874\n","epoch 46\n","TRAIN\tloss:1.0777, acc:62.5351, f1:39.0322, corr:0.5485\n","DEV  \tloss:1.1165, acc:60.5000, f1:28.9296, corr:0.5746\n","epoch 47\n","TRAIN\tloss:1.0649, acc:61.2722, f1:37.4692, corr:0.5546\n","DEV  \tloss:1.0809, acc:62.9000, f1:33.0889, corr:0.5865\n","epoch 48\n","TRAIN\tloss:1.0842, acc:62.3480, f1:38.4748, corr:0.5506\n","DEV  \tloss:1.0889, acc:62.4000, f1:33.8371, corr:0.5561\n","epoch 49\n","TRAIN\tloss:1.0911, acc:61.4125, f1:37.4493, corr:0.5494\n","DEV  \tloss:1.1146, acc:60.9000, f1:32.6059, corr:0.5564\n","epoch 50\n","TRAIN\tloss:1.0737, acc:62.0206, f1:37.0328, corr:0.5398\n","DEV  \tloss:1.0866, acc:61.9000, f1:34.4874, corr:0.5617\n","epoch 51\n","TRAIN\tloss:1.0786, acc:61.2254, f1:36.8262, corr:0.5452\n","DEV  \tloss:1.0667, acc:61.9000, f1:32.1391, corr:0.5844\n","epoch 52\n","TRAIN\tloss:1.0519, acc:62.1141, f1:38.9351, corr:0.5517\n","DEV  \tloss:1.0863, acc:61.6000, f1:34.3647, corr:0.5798\n","epoch 53\n","TRAIN\tloss:1.0572, acc:62.3012, f1:38.3495, corr:0.5478\n","DEV  \tloss:1.1063, acc:61.6000, f1:34.4058, corr:0.5487\n","epoch 54\n","TRAIN\tloss:1.0761, acc:61.0384, f1:37.2852, corr:0.5470\n","DEV  \tloss:1.1064, acc:60.6000, f1:30.8372, corr:0.5875\n","epoch 55\n","TRAIN\tloss:1.0673, acc:62.5351, f1:39.0642, corr:0.5470\n","DEV  \tloss:1.0796, acc:62.1000, f1:33.8132, corr:0.5843\n","epoch 56\n","TRAIN\tloss:1.0685, acc:61.5061, f1:38.2022, corr:0.5450\n","DEV  \tloss:1.0793, acc:62.0000, f1:33.9800, corr:0.5792\n","epoch 57\n","TRAIN\tloss:1.0409, acc:62.9560, f1:40.6278, corr:0.5497\n","DEV  \tloss:1.0990, acc:61.4000, f1:33.6526, corr:0.5530\n","epoch 58\n","TRAIN\tloss:1.0741, acc:61.0851, f1:39.0973, corr:0.5274\n","DEV  \tloss:1.0877, acc:61.4000, f1:31.1750, corr:0.5750\n","epoch 59\n","TRAIN\tloss:1.0456, acc:62.5819, f1:39.5622, corr:0.5410\n","DEV  \tloss:1.0834, acc:62.5000, f1:35.2097, corr:0.5697\n","epoch 60\n","TRAIN\tloss:1.0454, acc:63.7044, f1:39.6896, corr:0.5494\n","DEV  \tloss:1.0955, acc:61.6000, f1:29.6696, corr:0.5848\n","epoch 61\n","TRAIN\tloss:1.0638, acc:61.8335, f1:39.4612, corr:0.5420\n","DEV  \tloss:1.0980, acc:61.6000, f1:35.1942, corr:0.5732\n","epoch 62\n","TRAIN\tloss:1.0379, acc:62.9093, f1:38.7359, corr:0.5471\n","DEV  \tloss:1.1276, acc:61.5000, f1:31.6724, corr:0.5455\n","epoch 63\n","TRAIN\tloss:1.0418, acc:62.4415, f1:39.2614, corr:0.5469\n","DEV  \tloss:1.0949, acc:62.2000, f1:34.5208, corr:0.5567\n","epoch 64\n","TRAIN\tloss:1.0311, acc:63.2834, f1:41.1405, corr:0.5403\n","DEV  \tloss:1.1054, acc:60.9000, f1:29.4238, corr:0.5763\n","epoch 65\n","TRAIN\tloss:1.0532, acc:62.3012, f1:40.2095, corr:0.5365\n","DEV  \tloss:1.1063, acc:61.5000, f1:30.4813, corr:0.5430\n","epoch 66\n","TRAIN\tloss:1.0101, acc:64.5463, f1:42.3696, corr:0.5506\n","DEV  \tloss:1.0948, acc:62.2000, f1:32.9071, corr:0.5648\n","epoch 67\n","TRAIN\tloss:1.0517, acc:63.5173, f1:41.2612, corr:0.5324\n","DEV  \tloss:1.1184, acc:61.5000, f1:30.1813, corr:0.5763\n","epoch 68\n","TRAIN\tloss:1.0319, acc:63.3770, f1:40.1386, corr:0.5409\n","DEV  \tloss:1.0905, acc:61.6000, f1:32.8624, corr:0.5681\n","epoch 69\n","TRAIN\tloss:1.0394, acc:62.0674, f1:39.2678, corr:0.5451\n","DEV  \tloss:1.1135, acc:61.6000, f1:33.6117, corr:0.5418\n","epoch 70\n","TRAIN\tloss:1.0586, acc:62.7689, f1:39.5448, corr:0.5370\n","DEV  \tloss:1.0877, acc:62.1000, f1:31.9101, corr:0.5779\n","epoch 71\n","TRAIN\tloss:1.0224, acc:63.0964, f1:40.4682, corr:0.5507\n","DEV  \tloss:1.0994, acc:61.0000, f1:29.0619, corr:0.5788\n","epoch 72\n","TRAIN\tloss:1.0512, acc:61.9270, f1:40.0386, corr:0.5436\n","DEV  \tloss:1.1150, acc:61.7000, f1:34.7519, corr:0.5545\n","epoch 73\n","TRAIN\tloss:1.0280, acc:62.5819, f1:38.5550, corr:0.5491\n","DEV  \tloss:1.1031, acc:62.1000, f1:33.2800, corr:0.5600\n","epoch 74\n","TRAIN\tloss:1.0380, acc:64.4060, f1:42.5694, corr:0.5417\n","DEV  \tloss:1.1388, acc:61.9000, f1:33.8412, corr:0.5547\n","epoch 75\n","TRAIN\tloss:1.0415, acc:63.7044, f1:41.7276, corr:0.5505\n","DEV  \tloss:1.1030, acc:61.6000, f1:31.5399, corr:0.5784\n","epoch 76\n","TRAIN\tloss:1.0413, acc:62.7689, f1:39.2601, corr:0.5477\n","DEV  \tloss:1.0853, acc:61.3000, f1:32.8573, corr:0.5816\n","epoch 77\n","TRAIN\tloss:1.0281, acc:62.8625, f1:40.1564, corr:0.5466\n","DEV  \tloss:1.0970, acc:61.2000, f1:29.8775, corr:0.5673\n","epoch 78\n","TRAIN\tloss:1.0148, acc:63.0028, f1:41.3922, corr:0.5483\n","DEV  \tloss:1.0937, acc:62.9000, f1:35.9820, corr:0.5949\n","epoch 79\n","TRAIN\tloss:1.0188, acc:63.8447, f1:42.9307, corr:0.5473\n","DEV  \tloss:1.1120, acc:61.6000, f1:33.7482, corr:0.5613\n","epoch 80\n","TRAIN\tloss:1.0089, acc:65.0140, f1:44.7861, corr:0.5513\n","DEV  \tloss:1.1084, acc:61.9000, f1:31.6427, corr:0.5605\n","epoch 81\n","TRAIN\tloss:1.0383, acc:63.8447, f1:41.4603, corr:0.5391\n","DEV  \tloss:1.0992, acc:61.3000, f1:30.3597, corr:0.5652\n","epoch 82\n","TRAIN\tloss:1.0446, acc:63.1431, f1:41.7586, corr:0.5378\n","DEV  \tloss:1.0879, acc:60.9000, f1:28.5125, corr:0.5683\n","epoch 83\n","TRAIN\tloss:1.0293, acc:62.8625, f1:39.2364, corr:0.5468\n","DEV  \tloss:1.0801, acc:63.1000, f1:34.5579, corr:0.5955\n","epoch 84\n","TRAIN\tloss:1.0416, acc:63.5173, f1:41.4060, corr:0.5444\n","DEV  \tloss:1.1232, acc:61.6000, f1:32.2041, corr:0.5619\n","epoch 85\n","TRAIN\tloss:1.0341, acc:62.4415, f1:40.8457, corr:0.5457\n","DEV  \tloss:1.1031, acc:61.5000, f1:33.4093, corr:0.5797\n","epoch 86\n","TRAIN\tloss:1.0375, acc:63.1899, f1:42.2424, corr:0.5431\n","DEV  \tloss:1.0970, acc:62.3000, f1:33.0913, corr:0.5605\n","epoch 87\n","TRAIN\tloss:1.0309, acc:63.0028, f1:40.2108, corr:0.5452\n","DEV  \tloss:1.0977, acc:61.8000, f1:34.7206, corr:0.5720\n","epoch 88\n","TRAIN\tloss:1.0316, acc:61.8335, f1:38.1927, corr:0.5386\n","DEV  \tloss:1.1146, acc:61.4000, f1:30.7674, corr:0.5609\n","epoch 89\n","TRAIN\tloss:1.0035, acc:63.9850, f1:42.3163, corr:0.5412\n","DEV  \tloss:1.1126, acc:61.2000, f1:31.2135, corr:0.5647\n","epoch 90\n","TRAIN\tloss:1.0207, acc:63.1899, f1:40.8429, corr:0.5391\n","DEV  \tloss:1.1223, acc:62.2000, f1:33.9656, corr:0.5547\n","epoch 91\n","TRAIN\tloss:1.0263, acc:62.4883, f1:40.2009, corr:0.5395\n","DEV  \tloss:1.1085, acc:62.6000, f1:35.7591, corr:0.5729\n","epoch 92\n","TRAIN\tloss:1.0228, acc:63.2834, f1:41.9237, corr:0.5505\n","DEV  \tloss:1.1076, acc:62.1000, f1:34.0936, corr:0.5498\n","epoch 93\n","TRAIN\tloss:1.0126, acc:62.3012, f1:40.6723, corr:0.5315\n","DEV  \tloss:1.1194, acc:61.4000, f1:32.4118, corr:0.5639\n","epoch 94\n","TRAIN\tloss:0.9958, acc:64.5463, f1:41.5812, corr:0.5508\n","DEV  \tloss:1.1320, acc:61.3000, f1:32.5848, corr:0.5703\n","epoch 95\n","TRAIN\tloss:1.0212, acc:63.4238, f1:40.7786, corr:0.5506\n","DEV  \tloss:1.1144, acc:61.6000, f1:32.4156, corr:0.5593\n","epoch 96\n","TRAIN\tloss:0.9930, acc:64.7802, f1:43.3898, corr:0.5463\n","DEV  \tloss:1.1428, acc:62.9000, f1:34.4577, corr:0.5577\n","epoch 97\n","TRAIN\tloss:1.0075, acc:63.6576, f1:42.5432, corr:0.5358\n","DEV  \tloss:1.1221, acc:61.1000, f1:31.0569, corr:0.5482\n","epoch 98\n","TRAIN\tloss:1.0042, acc:65.0608, f1:43.3136, corr:0.5435\n","DEV  \tloss:1.1255, acc:62.2000, f1:35.4396, corr:0.5749\n","epoch 99\n","TRAIN\tloss:0.9759, acc:63.9383, f1:43.5585, corr:0.5445\n","DEV  \tloss:1.1273, acc:61.4000, f1:33.9380, corr:0.5684\n","====Train End====\n","best acc:63.1000\n","====Test====\n","TEST\tacc:64.3275, f1:35.3958, corr:0.6045\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(64.32748538011695, 35.395789442382764, 0.6045400572427136)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"QaO7bqXSU9gY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}